{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1) Create a Tf-IDF matrix for the following documents.\n",
        "\n",
        "    a)   'the man went out for a walk'\n",
        "    \n",
        "    b)   'the children sat around the fire'\n",
        "    \n"
      ],
      "metadata": {
        "id": "MVzp3Q6tItia"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYIrkWA9Isrw",
        "outputId": "ff62bdcd-34dc-4f5b-c9d3-38e2ec8cc743"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 2 1 1 0 1 1 1]\n",
            " [1 1 1 0 0 0 1 2 0 0]]\n",
            "['around' 'children' 'fire' 'for' 'man' 'out' 'sat' 'the' 'walk' 'went']\n"
          ]
        }
      ],
      "source": [
        "# Import CountVectorizer from scikit-learn\n",
        "# CountVectorizer converts text documents into a matrix of token counts (Bag-of-Words model)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Define the two text documents\n",
        "document1 = 'the man went out for a walk for'\n",
        "document2 = 'the children sat around the fire'\n",
        "\n",
        "# Initialize CountVectorizer\n",
        "# This will break text into tokens (words) and count how often each appears\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit the vectorizer to the documents and transform them into a term-document matrix\n",
        "# Each row = a document\n",
        "# Each column = a unique word (token) from the whole corpus\n",
        "# Each value = frequency of that word in the document\n",
        "tf_matrix = vectorizer.fit_transform([document1, document2])\n",
        "\n",
        "# Convert the sparse matrix (compressed format) into a dense array for easier reading\n",
        "tf_array = tf_matrix.toarray()\n",
        "\n",
        "# Print the array\n",
        "# Example:\n",
        "# [[1 0 0 1 ...]   -> word counts for document1\n",
        "#  [0 1 1 0 ...]]  -> word counts for document2\n",
        "print(tf_array)\n",
        "\n",
        "# If you want to see which words correspond to the columns, you can print:\n",
        "print(vectorizer.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OfaeIWWZNG6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "W76c5DwnNAkP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import TfidfVectorizer from scikit-learn\n",
        "# TF-IDF = Term Frequency â€“ Inverse Document Frequency\n",
        "# It not only counts words but also reduces the weight of common words\n",
        "# (like 'the', 'for') while giving importance to unique words\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np   # for rounding values when printing\n",
        "\n",
        "# Define the two text documents\n",
        "document1 = 'the man went out for a walk'\n",
        "document2 = 'the children sat around the fire'\n",
        "\n",
        "# Initialize TfidfVectorizer\n",
        "# This will tokenize the documents, calculate TF-IDF for each word\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer on the documents and transform them into a TF-IDF matrix\n",
        "# Each row = document\n",
        "# Each column = unique word across all documents\n",
        "# Each value = TF-IDF score of that word in the document\n",
        "tfidf_matrix = vectorizer.fit_transform([document1, document2])\n",
        "\n",
        "# Convert sparse matrix into a dense array\n",
        "tfidf_array = tfidf_matrix.toarray()\n",
        "\n",
        "# Print the TF-IDF matrix rounded to 2 decimal places for readability\n",
        "print(np.round(tfidf_array, 2))\n",
        "\n",
        "# Print the feature (word) names to understand the column order\n",
        "print(vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIa8hU1FNBex",
        "outputId": "64aefe4e-c758-455b-cead-f409fb084781"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.   0.   0.   0.43 0.43 0.43 0.   0.3  0.43 0.43]\n",
            " [0.41 0.41 0.41 0.   0.   0.   0.41 0.58 0.   0.  ]]\n",
            "['around' 'children' 'fire' 'for' 'man' 'out' 'sat' 'the' 'walk' 'went']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Find the Cosine Similarity for above documents."
      ],
      "metadata": {
        "id": "MnWXg3eTNgoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import TfidfVectorizer for TF-IDF calculation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Import cosine_similarity to measure similarity between vectors\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Define two documents\n",
        "document1 = 'the man went out for a walk'\n",
        "document2 = 'the children sat around the fire'\n",
        "\n",
        "# Step 1: Initialize TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Step 2: Fit and transform documents into TF-IDF matrix\n",
        "# Each row = a document, each column = a unique word across all documents\n",
        "tfidf_matrix = vectorizer.fit_transform([document1, document2])\n",
        "\n",
        "# Step 3: Compute cosine similarity between the two documents\n",
        "# tfidf_matrix[0] = vector for document1\n",
        "# tfidf_matrix[1] = vector for document2\n",
        "cosine_sim = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])\n",
        "\n",
        "# Step 4: Print similarity score\n",
        "print(cosine_sim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MahyW4-PNieG",
        "outputId": "5ba32532-d079-43ad-cb54-d995e97d29a3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.17578608]]\n"
          ]
        }
      ]
    }
  ]
}