{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ez_jCL0G4plj",
        "outputId": "e399dbc0-3101-44db-dd3f-5646a75cb10c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part A\n",
        "\n"
      ],
      "metadata": {
        "id": "JonVYZi64g6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Lowercasing\n"
      ],
      "metadata": {
        "id": "cB04NmdF46RL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Natural Language Processing Is Fun!\"\n",
        "text = text.lower()\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuMuUT3_5vpB",
        "outputId": "01ce1e53-1a55-44ce-ca40-76ab3be4d2cf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "natural language processing is fun!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifjPoTNM4Ge5",
        "outputId": "c30f8b62-1817-4c61-f14a-33e8e1e5369b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natural', 'language', 'processing', 'nlp', 'is', 'transforming', 'the', 'way', 'humans', 'interac']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import string\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "text = \"Natural Language Processing (NLP) is transforming the way humans interac\"\n",
        "text = text.lower()\n",
        "text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Removing Punctuation"
      ],
      "metadata": {
        "id": "Mkgs-rLi5gTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "text = \"Hello, world! NLP is exciting.\"\n",
        "text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "print(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIs9SzPs5ph-",
        "outputId": "e5134e7d-1c8a-46cd-b1ab-9fca0a99ed11"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello world NLP is exciting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Tokenization"
      ],
      "metadata": {
        "id": "ErmBzVS86w3z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization is the process of breaking down a sequence of text into smaller units called tokens, which can be words, sub-words, or characters"
      ],
      "metadata": {
        "id": "ViNnHsoQ645j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClMPauXm6jbx",
        "outputId": "e02d574c-6a68-4c89-b291-7570cb2ab115"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "text = \"Tokenize this sentence.\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPnH0BOh7vAf",
        "outputId": "50026201-49a8-4524-9bb0-d51243e18623"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tokenize', 'this', 'sentence', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Stopword Removal"
      ],
      "metadata": {
        "id": "dLlJKxF19vEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered = [word for word in tokens if word.lower() not in stop_words]\n",
        "print(filtered)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CK34OwvE75CC",
        "outputId": "de0e38b9-512e-4655-b4c8-5454a847ba04"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tokenize', 'sentence', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Stemming"
      ],
      "metadata": {
        "id": "-smjQr3d-CzQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The process of reducing words to their root or base form, known as a stem"
      ],
      "metadata": {
        "id": "HAKv03zp-OHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "words = [\"playing\", \"played\", \"player\", ]\n",
        "stemmed = [stemmer.stem(word) for word in words]\n",
        "print(stemmed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eK5ajRxV-TAa",
        "outputId": "df8b0e80-2767-4c6b-f3dc-11156d308145"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['play', 'play', 'player']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Lemmatization"
      ],
      "metadata": {
        "id": "V2t9-vST-bXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A process in Natural Language Processing (NLP) that reduces words to their base or dictionary form, known as the lemma"
      ],
      "metadata": {
        "id": "-vqQIQdE-ePV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = [\"playing\", \"better\", \"running\"]\n",
        "lemmatized = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
        "print(lemmatized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QyMdwH2-px7",
        "outputId": "04fe74cf-37ac-45d0-8876-b3397f04a6ac"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['play', 'better', 'run']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part B"
      ],
      "metadata": {
        "id": "lDYahAKO8FGS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the text corpus:\n",
        "\"Natural Language Processing (NLP) is transforming the way humans interact with\n",
        "machines."
      ],
      "metadata": {
        "id": "RZuVY2p78Ln6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "01) Perform the following:\n",
        "*   Convert the text to lowercase\n",
        "*   Remove punctuation\n",
        "*   Tokenize the sentence into words\n",
        "\n",
        "02) From the tokenized output of Task 1, remove all stopwords using NLTK.\n",
        "\n",
        "03) Apply stemming to the filtered tokens using the PorterStemmer.\n",
        "\n",
        "04) Apply lemmatization to the original tokens using WordNetLemmatizer and compare results with stemming."
      ],
      "metadata": {
        "id": "dg-c9vvD8dZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert the text to lowercase\n",
        "import nltk\n",
        "import string\n",
        "# nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "text = \"Natural Language Processing (NLP) is transforming the way humans interactive\"\n",
        "text = text.lower()\n",
        "text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1p1zjmun84nO",
        "outputId": "ad3d9b60-e3c9-42ac-bafc-27b9b3e09d29"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natural', 'language', 'processing', 'nlp', 'is', 'transforming', 'the', 'way', 'humans', 'interactive']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8ArwHUU9ILa",
        "outputId": "db3bb5cd-36d9-435d-a2c8-42fbeec2a63d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "natural language processing nlp is transforming the way humans interactive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove punctuation\n",
        "from nltk.corpus import stopwords\n",
        "# nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdPY0bKo_AdX",
        "outputId": "f93b5904-c0d9-4913-b915-a810a58520a3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natural', 'language', 'processing', 'nlp', 'transforming', 'way', 'humans', 'interactive']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize the sentence into words\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "print(stemmed_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auWV81TYAMAL",
        "outputId": "0f21d4af-840d-4815-f236-1c09b80069aa"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natur', 'languag', 'process', 'nlp', 'transform', 'way', 'human', 'interact']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "# nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word, pos='v') for word in filtered_tokens]\n",
        "print(lemmatized_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogfm0i4XBKtT",
        "outputId": "0acd110c-f3b6-4359-9e95-2f400e882eec"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natural', 'language', 'process', 'nlp', 'transform', 'way', 'humans', 'interactive']\n"
          ]
        }
      ]
    }
  ]
}